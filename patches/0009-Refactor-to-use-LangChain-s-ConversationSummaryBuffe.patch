From ce3c2b5258ccb713c5c875b31220c66416bb6b2c Mon Sep 17 00:00:00 2001
From: Den <2119348+dzianisv@users.noreply.github.com>
Date: Tue, 22 Jul 2025 13:09:57 +0300
Subject: [PATCH 09/91] Refactor to use LangChain's
 ConversationSummaryBufferMemory
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Replaces custom conversation summarization with LangChain's battle-tested
memory system that provides:

- Intelligent AI-powered summarization using the same LLM
- Automatic token-aware memory management (4000 token limit)
- Seamless integration with LangChain message flow
- Professional conversation persistence across iterations
- Reduced custom code complexity and maintenance burden

Benefits over custom implementation:
- LLM understands conversation context better than string parsing
- Built-in token counting and management
- Battle-tested edge case handling
- Contextual intelligence in summarization decisions

This eliminates "not invented here" syndrome and leverages proven
LangChain functionality for more robust conversation handling.

ðŸ¤– Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>
---
 chrome/browser/resources/vibe/ai_agent.js | 106 ++++++++--------------
 1 file changed, 40 insertions(+), 66 deletions(-)

diff --git a/chrome/browser/resources/vibe/ai_agent.js b/chrome/browser/resources/vibe/ai_agent.js
index 85d9ca7d9b..cdbd64d95a 100644
--- a/chrome/browser/resources/vibe/ai_agent.js
+++ b/chrome/browser/resources/vibe/ai_agent.js
@@ -6,6 +6,7 @@
 import { initChatModel } from "langchain/chat_models/universal";
 import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
 import { HumanMessage, AIMessage, SystemMessage, ToolMessage } from "@langchain/core/messages";
+import { ConversationSummaryBufferMemory } from "langchain/memory";
 import { browserTools as extensionBrowserTools } from "./ai_tools.extension.js";
 
 /**
@@ -23,57 +24,7 @@ import { browserTools as extensionBrowserTools } from "./ai_tools.extension.js";
 export class VibeLangchainAgent {
   constructor() {
     this.tools = extensionBrowserTools;
-  }
-
-  /**
-   * Summarize conversation to prevent token explosion
-   * @param {Array} messages - Array of conversation messages
-   * @param {number} maxMessages - Maximum messages to keep before summarization
-   * @returns {Array} Summarized messages
-   */
-  summarizeConversation(messages, maxMessages = 20) {
-    if (messages.length <= maxMessages) return messages;
-    
-    // Keep system message and recent messages
-    const systemMessage = messages[0];
-    const recentMessages = messages.slice(-maxMessages + 2); // Leave room for summary
-    
-    // Extract key information from older messages
-    const olderMessages = messages.slice(1, -maxMessages + 2);
-    const toolResults = [];
-    const userRequests = [];
-    const completedActions = [];
-    
-    for (const msg of olderMessages) {
-      if (msg instanceof HumanMessage) {
-        // Extract user requests
-        userRequests.push(msg.content);
-      } else if (msg instanceof ToolMessage) {
-        // Extract successful tool results
-        toolResults.push(`${msg.name}: ${msg.content.substring(0, 100)}...`);
-      } else if (msg instanceof AIMessage && msg.tool_calls) {
-        // Extract completed actions
-        msg.tool_calls.forEach(call => {
-          completedActions.push(`${call.name}: ${JSON.stringify(call.args).substring(0, 60)}...`);
-        });
-      }
-    }
-    
-    // Create summary message
-    const summary = `CONVERSATION SUMMARY (${olderMessages.length} messages compressed):
-User requests: ${userRequests.slice(-3).join('; ')}
-Completed actions: ${completedActions.slice(-5).join('; ')}
-Key tool results: ${toolResults.slice(-5).join('; ')}
-
-Continuing with recent conversation...`;
-    
-    console.log(`ðŸ’¾ [AI_AGENT] Conversation summarized: ${olderMessages.length} â†’ 1 message, ~${Math.floor(olderMessages.length * 50)}% token reduction`);
-    
-    return [
-      systemMessage,
-      new HumanMessage(summary),
-      ...recentMessages
-    ];
+    this.memory = null; // Will be initialized when model is available
   }
 
   /**
@@ -161,6 +112,17 @@ Continuing with recent conversation...`;
 
     const llm = await initChatModel(modelName, modelConfig);
 
+    // Initialize LangChain memory with intelligent summarization
+    if (!this.memory) {
+      this.memory = new ConversationSummaryBufferMemory({
+        llm: llm, // Use same model for intelligent summarization
+        maxTokenLimit: 4000, // Token threshold for summarization
+        returnMessages: true,
+        memoryKey: "chat_history"
+      });
+      console.log(`ðŸ§  [AI_AGENT] Initialized LangChain memory with ${this.memory.maxTokenLimit} token limit`);
+    }
+
     // Bind tools to the model for function calling
     const llmWithTools = llm.bind({ tools: langchainTools });
 
@@ -177,11 +139,16 @@ Continuing with recent conversation...`;
 Example first call:
 {"tool_calls":[{"name":"reasoning","args":{"thinking":"I need to...","next_goal":"..."}}]}`;
 
-    // Create conversation with system message
+    // Get conversation history from LangChain memory and add current request
+    const chatHistory = await this.memory.chatHistory.getMessages();
     const messages = [
       new SystemMessage(systemPrompt),
+      ...chatHistory,
       new HumanMessage(user_request)
     ];
+    
+    // Add the current user message to memory
+    await this.memory.chatHistory.addMessage(new HumanMessage(user_request));
 
     const maxIterations = config.maxIterations || 32;
     const maxTokens = config.maxTokens || 100000; // Token budget guard
@@ -231,14 +198,6 @@ Example first call:
         };
       }
 
-      // Apply conversation summarization at regular intervals or when approaching token limits
-      const shouldSummarize = messages.length > 30 || 
-                             (totalTokensUsed > maxTokens * 0.7 && messages.length > 15);
-      
-      if (shouldSummarize) {
-        messages.splice(0, messages.length, ...this.summarizeConversation(messages));
-      }
-
       try {
         const result = await llmWithTools.invoke(messages);
         
@@ -274,7 +233,9 @@ Example first call:
               arguments: JSON.stringify(toolCall.args)
             }
           }));
-          messages.push(new AIMessage(result.content, { tool_calls: formattedToolCalls }));
+          const aiMessage = new AIMessage(result.content, { tool_calls: formattedToolCalls });
+          messages.push(aiMessage);
+          await this.memory.chatHistory.addMessage(aiMessage);
           
           // Execute each tool call
           for (const toolCall of result.tool_calls) {
@@ -283,7 +244,9 @@ Example first call:
             if (!tool) {
               console.error(`âŒ [AI_AGENT] Tool not found: ${toolCall.name}`);
               const errorMsg = `Tool not found: ${toolCall.name}`;
-              messages.push(new ToolMessage(errorMsg, toolCall.id, toolCall.name));
+              const errorToolMessage = new ToolMessage(errorMsg, toolCall.id, toolCall.name);
+              messages.push(errorToolMessage);
+              await this.memory.chatHistory.addMessage(errorToolMessage);
               continue;
             }
 
@@ -328,7 +291,9 @@ Example first call:
               
               // Add tool message to conversation - ensure it's a string and include tool name
               const resultString = typeof toolResult === 'string' ? toolResult : JSON.stringify(toolResult);
-              messages.push(new ToolMessage(resultString, toolCall.id, toolCall.name));
+              const toolMessage = new ToolMessage(resultString, toolCall.id, toolCall.name);
+              messages.push(toolMessage);
+              await this.memory.chatHistory.addMessage(toolMessage);
               
               consecutiveFailures = 0; // Reset on successful tool execution
             } catch (toolError) {
@@ -341,7 +306,9 @@ Example first call:
                 args: toolCall.args,
                 result: { error: errorMsg }
               });
-              messages.push(new ToolMessage(errorMsg, toolCall.id, toolCall.name));
+              const errorToolMessage = new ToolMessage(errorMsg, toolCall.id, toolCall.name);
+              messages.push(errorToolMessage);
+              await this.memory.chatHistory.addMessage(errorToolMessage);
               
               // Add context message after multiple failures by including it in the next system message
               if (consecutiveFailures >= 3) {
@@ -355,6 +322,11 @@ Example first call:
           // No more tool calls - AI is done
           console.log(`âœ… [AI_AGENT] Processing complete after ${iteration} iterations`);
           console.log(`Final response: ${result.content}`);
+          
+          // Add final AI response to memory
+          const finalMessage = new AIMessage(result.content);
+          await this.memory.chatHistory.addMessage(finalMessage);
+          
           return { 
             output: result.content,
             reasoning: result.content,
@@ -365,9 +337,11 @@ Example first call:
         
         // Add failure context to next iteration if needed
         if (consecutiveFailures >= 3) {
-          messages.push(new HumanMessage(
+          const advisoryMessage = new HumanMessage(
             `SYSTEM ADVISORY: ${consecutiveFailures} consecutive tool failures detected. Consider alternative approaches, verify tab context, or explain current limitations. Focus on recovery strategies.`
-          ));
+          );
+          messages.push(advisoryMessage);
+          await this.memory.chatHistory.addMessage(advisoryMessage);
           consecutiveFailures = 0; // Reset after adding advisory
         }
         
-- 
2.50.0

